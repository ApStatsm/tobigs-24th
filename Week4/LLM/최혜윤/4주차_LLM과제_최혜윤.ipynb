{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Transformer Architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "vzEqPC5PATwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention 계산식을 생각하며 빈칸을 채워봅시다!!"
      ],
      "metadata": {
        "id": "2q4Y7T3NCStq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# A. Scaled Dot-Product Attention\n",
        "# -------------------------\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        \"\"\"\n",
        "        Q,K,V: (batch, heads, seq_len, d_k)\n",
        "        mask:  (batch, 1 or heads, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        d_k = Q.size(-1)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (B,H,L,L)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, V)  # (B,H,L,d_k)\n",
        "        return output, attn\n"
      ],
      "metadata": {
        "id": "wLxpDVbargxb",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 1) 아래 코드를 살펴보고, 단순 Attention 대신 Multi-Head Attention을 사용하는 이유를 설명하시오.\n",
        "\n",
        "Multi-Head Attention은 입력 시퀀스 내 단어 간 관계를 다양한 시각에서 파악해 모델의 성능을 높인다.\n",
        "\n",
        "문제 2) Positional Encoding의 기능에 대해 설명하시오.\n",
        "\n",
        "Positional Encoding은 위치 정보가 없는 Transformer에 순서 정보를 주입하여 문맥 이해를 가능하게 한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "hXJz80qHmRAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# B. Multi-Head Attention\n",
        "# -------------------------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.attn = ScaledDotProductAttention(dropout=dropout)\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        B, L, D = x.shape\n",
        "        x = x.view(B, L, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def _combine_heads(self, x):\n",
        "        B, H, L, d_k = x.shape\n",
        "        x = x.transpose(1, 2).contiguous().view(B, L, H * d_k)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, kv=None, mask=None):\n",
        "        residual = x\n",
        "        if kv is None:\n",
        "            kv = x\n",
        "        Q = self._split_heads(self.W_q(x))\n",
        "        K = self._split_heads(self.W_k(kv))\n",
        "        V = self._split_heads(self.W_v(kv))\n",
        "        ctx, _ = self.attn(Q, K, V, mask=mask)          # (B,H,L_q,d_k)\n",
        "        out = self._combine_heads(ctx)                  # (B,L_q,D)\n",
        "        out = self.dropout(self.W_o(out))\n",
        "        return self.layernorm(out + residual)\n",
        "\n",
        "# -------------------------\n",
        "# C. Positional Encoding (sin/cos)\n",
        "# -------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-(math.log(10000.0) / d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # (1,L,D)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.dtype)\n",
        "        return self.dropout(x)\n",
        "\n",
        "# -------------------------\n",
        "# D. Position-wise FFN\n",
        "# -------------------------\n",
        "class PositionwiseFFN(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.net(x)\n",
        "        return self.norm(x + residual)"
      ],
      "metadata": {
        "id": "VFSHAx-rgKwo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer의 인코더와 디코더 레이어 구조를 생각하며 빈칸을 채워봅시다!!"
      ],
      "metadata": {
        "id": "b97cQEvcm76n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# E. Encoder\n",
        "# -------------------------\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        x = self.self_attn(x, kv=None, mask=src_mask)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.posenc = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        x = self.embed(src) * math.sqrt(d_model := self.embed.embedding_dim)\n",
        "        x = self.posenc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask=src_mask)\n",
        "        return x\n",
        "\n",
        "# -------------------------\n",
        "# F. Decoder\n",
        "# -------------------------\n",
        "def generate_subsequent_mask(sz: int):\n",
        "    mask = torch.tril(torch.ones(sz, sz)).bool()\n",
        "    return mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ffn = PositionwiseFFN(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.self_attn(x, kv=None, mask=tgt_mask)\n",
        "        x = self.cross_attn(x, kv=enc_out, mask=memory_mask)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, num_heads, d_ff, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.posenc = PositionalEncoding(d_model, max_len, dropout)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, memory_mask=None):\n",
        "        x = self.embed(tgt) * math.sqrt(d_model := self.embed.embedding_dim)\n",
        "        x = self.posenc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# -------------------------\n",
        "# G. 전체 Transformer + 마스크\n",
        "# -------------------------\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, d_model=256, N=4, heads=4, d_ff=1024, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab, d_model, N, heads, d_ff, dropout, max_len)\n",
        "        self.generator = nn.Linear(d_model, tgt_vocab)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src != PAD).unsqueeze(1).unsqueeze(1)  # (B,1,1,Ls)\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        B, L = tgt.shape\n",
        "        pad = (tgt != PAD).unsqueeze(1).unsqueeze(1)   # (B,1,1,Lt)\n",
        "        causal = generate_subsequent_mask(L).to(tgt.device)  # (1,1,Lt,Lt)\n",
        "        return pad & causal\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        memory = self.encoder(src, src_mask=src_mask)\n",
        "        out = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=src_mask)\n",
        "        logits = self.generator(out)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "V6nhDAo0gXGp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Self-Supervised Learning"
      ],
      "metadata": {
        "id": "LvV1GzlY-927"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 문제 1) Autoencoding Language Model\n",
        "아래 세 문장에서 BERT가 [MASK] 위치에 대해 예측한 1순위 토큰이 문맥상 적절한지 평가하세요.\n",
        "\n",
        "적절하다면, 왜 해당 토큰이 자연스럽다고 볼 수 있는지 근거를 제시하세요.\n",
        "\n",
        "적절하지 않다면, 그 이유가 문맥 이해 부족 때문인지, 아니면 훈련 데이터 분포(자주 등장하는 표현) 때문인지 분석해 보세요.\n",
        "\n",
        "적절하다. 1번 답은 문법적, 의미적으로 자연스럽고, 2번 답도 의미적으로 자연스럽고, 3번 답도 전치사와의 호응이 이루어지고, 상황에 맞게 쓰였다."
      ],
      "metadata": {
        "id": "iZoqOG5j-GKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "mlm_name = \"distilbert-base-uncased\"  # 경량 BERT\n",
        "tok = AutoTokenizer.from_pretrained(mlm_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(mlm_name)\n",
        "model.eval()\n",
        "\n",
        "def topk_mask_fill(text, k=5):\n",
        "    inputs = tok(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    mask_idx = (inputs.input_ids[0] == tok.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
        "    probs = torch.softmax(logits[0, mask_idx], dim=-1)\n",
        "    topk_ids = torch.topk(probs, k=k).indices.tolist()\n",
        "    return [(tok.decode([i]), float(probs[i])) for i in topk_ids]\n",
        "\n",
        "sentences = [\n",
        "    \"I'm wondering if I should eat [MASK] for lunch today.\",\n",
        "    \"I decided to go to the [MASK] with my friends this weekend.\",\n",
        "    \"It started to rain and I remembered I left my umbrella at [MASK].\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    print(\"\\nInput:\", s)\n",
        "    preds = topk_mask_fill(s, k=5) # top-k 자유롭게 수정 가능\n",
        "    for t,p in preds:\n",
        "        print(f\"  - {t:15s}  p={p:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6T28myl7-ug",
        "outputId": "a69eee5a-cdc5-4309-e161-5aa23e93af3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input: I'm wondering if I should eat [MASK] for lunch today.\n",
            "  - something        p=0.0690\n",
            "  - here             p=0.0688\n",
            "  - breakfast        p=0.0414\n",
            "  - dinner           p=0.0405\n",
            "  - pizza            p=0.0374\n",
            "\n",
            "Input: I decided to go to the [MASK] with my friends this weekend.\n",
            "  - beach            p=0.1086\n",
            "  - movies           p=0.0727\n",
            "  - gym              p=0.0478\n",
            "  - mall             p=0.0325\n",
            "  - zoo              p=0.0305\n",
            "\n",
            "Input: It started to rain and I remembered I left my umbrella at [MASK].\n",
            "  - home             p=0.0831\n",
            "  - night            p=0.0602\n",
            "  - school           p=0.0366\n",
            "  - dawn             p=0.0308\n",
            "  - lunch            p=0.0271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Prompt Engineering"
      ],
      "metadata": {
        "id": "Crjl0OMiOMLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래는 동일한 질문에 대해 Baseline Prompt와 Engineered Prompt를 사용했을 때의 모델 답변이다.\n",
        "두 결과를 비교하고, 왜 프롬프트 엔지니어링(prompt engineering)이 중요한지 서술하시오.\n",
        "\n",
        "Baseline Prompt는“Convert … format” 정도로만 요청하였고, Engineered Prompt는“You are a date parser.”로 행동 맥락을 고정하였다.프롬프트를 잘 설계하면 정확도·일관성·파싱가능성이 크게 개선되기 때문에 프롬프트 엔지니어링이 중요하다.\n",
        "\n",
        "\n",
        "또한, 프롬프트 엔지니어링 기법에 대해 설명하시오.\n",
        "\n",
        "프롬프트 엔지니어링은 언어 모델에 원하는 출력이 나오도록 입력 프롬프트를 설계·최적화하는 기법이다. 같은 모델이라도 프롬프트를 어떻게 주느냐에 따라 결과 품질이 크게 달라질 수 있다."
      ],
      "metadata": {
        "id": "nW_Q66vormft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# -----------------------------\n",
        "# 1) 모델 로드\n",
        "# -----------------------------\n",
        "model_id = \"google/flan-t5-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(device)\n",
        "\n",
        "def generate(prompt, max_new_tokens=128):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) 예제 프롬프트들 (Baseline vs Engineered)\n",
        "# -----------------------------\n",
        "prompts = {\n",
        "    \"convert date\": {\n",
        "        \"baseline\": '''Convert March 5th, 2024 to YYYY-MM-DD format ''',\n",
        "\n",
        "        \"engineered\": '''You are a date parser.\n",
        "Task: Convert the input into exactly YYYY-MM-DD format (4-digit year, 2-digit month, 2-digit day).\n",
        "Rules:\n",
        "- Output ONLY the date in that format.\n",
        "- No extra text or explanation.\n",
        "Input: \"March 5th, 2024\"\n",
        "Output:'''\n",
        "    }\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) 실행 및 비교 출력\n",
        "# -----------------------------\n",
        "for task, variants in prompts.items():\n",
        "    print(\"=\"*80)\n",
        "    print(f\"📝 Task: {task}\")\n",
        "\n",
        "    for kind, prompt in variants.items():\n",
        "        output = generate(prompt)\n",
        "        print(f\"\\n--- {kind.upper()} Prompt ---\")\n",
        "        print(prompt)\n",
        "        print(f\"\\n👉 Model Output:\\n{output}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1kmHcdHc2p5",
        "outputId": "6f401120-e84a-46eb-ab20-5dcdb32b2cb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "📝 Task: convert date\n",
            "\n",
            "--- BASELINE Prompt ---\n",
            "Convert March 5th, 2024 to YYYY-MM-DD format \n",
            "\n",
            "👉 Model Output:\n",
            "5th, 2024\n",
            "\n",
            "\n",
            "--- ENGINEERED Prompt ---\n",
            "You are a date parser.\n",
            "Task: Convert the input into exactly YYYY-MM-DD format (4-digit year, 2-digit month, 2-digit day).\n",
            "Rules:\n",
            "- Output ONLY the date in that format.\n",
            "- No extra text or explanation.\n",
            "Input: \"March 5th, 2024\"\n",
            "Output:\n",
            "\n",
            "👉 Model Output:\n",
            "\"5/05/2024\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.RAG\n"
      ],
      "metadata": {
        "id": "1yPb-Rz8Ia49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_community sentence-transformers faiss-cpu transformers accelerate langchain-core langchain-upstage bitsandbytes"
      ],
      "metadata": {
        "id": "Oy1CGdZYJ-yK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 문제1) 아래 코드를 참고하여 RAG 프로세스를 서술해주세요\n",
        "\n",
        "#### 문제2) sample.md 를 업로드해서 아래 샘플 질문들을 입력해 결과를 출력해보세요.\n",
        "\n",
        "1. \"인공지능의 역사에서 튜링 테스트란 무엇인가요?\"\n",
        "2. \"딥러닝 혁명은 언제 시작되었나요?\"\n",
        "3. \"데이터 분석에 대해 설명하세요\"\n"
      ],
      "metadata": {
        "id": "AgSeT4dsFqqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, tempfile\n",
        "from google.colab import files\n",
        "from langchain_upstage import ChatUpstage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# 파일 업로드 함수\n",
        "def upload_file():\n",
        "    print(\"문서 파일을 업로드해주세요.\")\n",
        "    uploaded = files.upload()\n",
        "    file_path = list(uploaded.keys())[0]\n",
        "    print(f\"업로드 완료: {file_path}\")\n",
        "    return file_path\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Solar-Pro2 LLM 로드\n",
        "# -----------------------------\n",
        "chat = ChatUpstage(\n",
        "    api_key=\"up_*************************jMaZ\",\n",
        "    model=\"solar-pro2\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Colab RAG 시스템 정의\n",
        "# -----------------------------\n",
        "def colab_rag_system(file_path):\n",
        "    # 1. 문서 로드\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    print(f\"문서 로딩 완료: {len(documents)} 개의 문서\")\n",
        "\n",
        "    # 2. 문서 분할\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"문서 분할 완료: {len(texts)} 개의 청크\")\n",
        "\n",
        "    # 3. 임베딩 + 벡터저장소\n",
        "    print(\"임베딩 모델 로딩 중...\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    print(\"벡터 저장소 구축 중...\")\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "    # 4. 검색기 생성\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "    # 5. 프롬프트 템플릿\n",
        "    prompt_template = \"\"\"\n",
        "    다음 정보를 바탕으로 질문에 답해주세요.\n",
        "    만약 관련 내용이 없다면 \"관련 내용을 찾을 수 없습니다.\"라고 답해주세요.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    질문: {question}\n",
        "    답변:\n",
        "    \"\"\"\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # 6. RAG 파이프라인 구축\n",
        "    print(\"RAG 파이프라인 구축 중...\")\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=chat,                    # 여기서 Solar-Pro2 사용\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": PROMPT}\n",
        "    )\n",
        "\n",
        "    print(\"RAG 시스템 준비 완료!\")\n",
        "\n",
        "    # 7. 대화형 질의\n",
        "    while True:\n",
        "        query = input(\"\\n질문을 입력하세요 (종료하려면 'q' 입력): \")\n",
        "        if query.lower() == 'q':\n",
        "            break\n",
        "\n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "        print(\"\\n답변:\", result[\"result\"])\n",
        "\n",
        "\n",
        "    # 8. 벡터 저장소 저장\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        index_path = os.path.join(temp_dir, \"faiss_index\")\n",
        "        vectorstore.save_local(index_path)\n",
        "        print(f\"\\n인덱스를 '{index_path}'에 저장했습니다.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = upload_file()\n",
        "    colab_rag_system(file_path)"
      ],
      "metadata": {
        "id": "0QE4D2pvFoSv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "6e255fc7-7b22-4bcc-897f-f50c1f05b1c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 파일을 업로드해주세요.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7bc1c74c-ecc6-4c32-9bda-2c1d9352d080\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7bc1c74c-ecc6-4c32-9bda-2c1d9352d080\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-455837758.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mcolab_rag_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-455837758.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"문서 파일을 업로드해주세요.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"업로드 완료: {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}